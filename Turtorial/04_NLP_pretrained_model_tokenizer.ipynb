{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/envs/vision/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel,AutoBackbone, AutoFeatureExtractor, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "pipeline() conisit of a) preprocessing, b) model, c) postprocess. \n",
    "<b>\n",
    "In NLP, preprocess() refers to converting text to tokens and assigning ID. This is performed using tokenizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in NLP task first task is tokenizing the raw texts; for this we will be used\n",
    "model_identifier='google-bert/bert-base-uncased'\n",
    "tokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=\"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "tokenized_sequence=tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs of Above tokenizer\n",
    "\n",
    "1) **input_ids**: This refers to indices value of the tokenizers. Tokenized valued is converted to ids(int) using vocabulary index.\n",
    "2) **attention_mask**: Refers which tokens should be attended or not. We dont attend \"padded\" tokens, and give 0 value to it. \n",
    "3) **token_type_id**: identifies which sequence a token belongs to when there is more than one sequenc. For example in QA  we can assign one value to *Question* tokens, and other token values for *Answering* tokens.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can also do batch tokeniation; We can have option to padding=True (add 0 or pad to sequennce with less tokens. The number of tokens for all batch remains same)\n",
    "\n",
    "batch_Sequence=[\n",
    "    \"Hi, my name is Akash.\",\n",
    "    \"The quick brown fox jumps toward the lazy dog.\"\n",
    "]\n",
    "\n",
    "batch_tokenized_sequence=tokenizer(batch_Sequence, padding=True) ## length of each ids will be same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[101, 7632, 1010, 2026, 2171, 2003, 9875, 4095, 1012, 102, 0, 0], [101, 1996, 4248, 2829, 4419, 14523, 2646, 1996, 13971, 3899, 1012, 102]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input_ids\",batch_tokenized_sequence['input_ids'])\n",
    "print(\"attention_mask\",batch_tokenized_sequence['attention_mask']) ## we can observe attn_mask=0 value for padded tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids [[101, 7632, 1010, 2026, 2171, 2003, 9875, 4095, 1012, 102, 0, 0], [101, 1996, 4248, 2829, 4419, 14523, 2646, 1996, 13971, 3899, 1012, 102]]\n",
      "attention_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "## Sometimes some sequence might be too_long. So, trancuation is same as \"max_length\". If set to True, the sequence are trancuated to \"max_length\" as per the used model\n",
    "\n",
    "batch_tokenized_sequence=tokenizer(batch_Sequence,padding=True,truncation=True)\n",
    "\n",
    "print(\"input_ids\",batch_tokenized_sequence['input_ids'])\n",
    "print(\"attention_mask\",batch_tokenized_sequence['attention_mask']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[  101,  7632,  1010,  2026,  2171,  2003,  9875,  4095,  1012,   102,\n",
      "             0,     0],\n",
      "        [  101,  1996,  4248,  2829,  4419, 14523,  2646,  1996, 13971,  3899,\n",
      "          1012,   102]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "type check\n",
      "type(input_ids <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "## notice the returned is not yet in tensor; so let us return \"PyTorch\" tensors.\n",
    "batch_tokenized_sequence=tokenizer(batch_Sequence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "print(\"input_ids\",batch_tokenized_sequence['input_ids'])\n",
    "print(\"attention_mask\",batch_tokenized_sequence['attention_mask'])\n",
    "print('type check')\n",
    "\n",
    "print(\"type(input_ids\",type(batch_tokenized_sequence['input_ids']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids=batch_tokenized_sequence['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_tokens=tokenizer.batch_decode(batch_ids,skip_special_tokens=True) ## if single sentence then only decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi, my name is akash.', 'the quick brown fox jumps toward the lazy dog.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##sequence ids to sequesnce or sentence again \n",
    "decode_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revising the *forward* Parameters of Autotokenizer.fromPreTrained();\n",
    "\n",
    "1. inputs: can be single sequence or batch sequence\n",
    "2. padding: if set to True, will add padding \"0\"\n",
    "3. trancuation: if set to True, will trancuate the long_sequence to \"max_length\" as defined in the models. \n",
    "4. return_tensors: if \"pt\", return PyTorch tensor, if \"tf\" return tensorflow tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of AutoModelForSequenceClassification\n",
    "\n",
    "- **config**: This parameter takes in a configuration object or string. It's typically an instance of AutoConfig, which defines the architecture and other settings of the model. If you pass a string, it should be the shortcut name of a pre-trained model configuration.\n",
    "\n",
    "- **model**: This parameter takes in a model object or string. You can pass an instance of a pre-trained sequence classification model, such as BertForSequenceClassification, or a string representing the shortcut name of a pre-trained model.\n",
    "\n",
    "- **tokenizer**: This parameter takes in a tokenizer object or string. You can pass an instance of a tokenizer associated with the pre-trained model, like BertTokenizer, or a string representing the shortcut name of a pre-trained tokenizer.\n",
    "\n",
    "- **model_kwargs**: This parameter accepts any additional keyword arguments that you would pass to the model class constructor. These arguments vary depending on the specific model architecture you're using. For example, for BERT models, common kwargs might include **num_labels (number of output labels) or hidden_dropout_prob (dropout probability for hidden layers)**.\n",
    "\n",
    "- **tokenizer_kwargs**: This parameter accepts any additional keyword arguments that you would pass to the tokenizer class constructor. Similar to model_kwargs, these arguments vary based on the specific tokenizer.\n",
    "\n",
    "- **pretrained_model_name_or_path**: This parameter specifies the pre-trained model to load. It can be the name of the model (e.g., \"bert-base-uncased\") or a path to a directory containing model weights and configuration files.\n",
    "\n",
    "- **cache_dir**: This parameter specifies the directory where the pre-trained models will be cached if the models are accessed from remote locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of AutoModelForSequenceClassification.from_preTrained().forward()\n",
    "\n",
    "- **input_ids**: Tensor containing the token indices of the input sequences.\n",
    "- **attention_mask**: (Optional) Tensor containing indices specifying which tokens should be attended to and which should not (e.g., padding tokens).\n",
    "- **token_type_ids**: (Optional) Tensor containing token type embeddings indicating which segment each token belongs to (e.g., for sequence pairs).\n",
    "- **position_ids**: (Optional) Tensor containing indices indicating the position of each token in the input sequence.\n",
    "- **head_mask**: (Optional) Tensor specifying which heads to mask in the self-attention layers.\n",
    "- **inputs_embeds**: (Optional) Tensor containing the embeddings of the input sequences instead of token indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 440M/440M [01:01<00:00, 7.10MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForSequenceClassification.from_pretrained(model_identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## for pretraining with the same config (for easy understanding) we can change num_class (replace the last layer with new layers to be trained with)\n",
    "\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_identifier,num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model ## we can already see the change in the last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will learn detailly about AutoModel in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
